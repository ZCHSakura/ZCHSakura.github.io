<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
<script>
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163934312-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163934312-1');
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon_logosc/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logosc/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logosc/favicon-16x16.png">
  <link rel="mask-icon" href="/images/favicon_logosc/favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zchsakura.github.io","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideRightBigIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="记录网络模型的各种优化">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习笔记(四)---优化方法理解">
<meta property="og:url" content="zchsakura.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)---%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%90%86%E8%A7%A3/index.html">
<meta property="og:site_name" content="周小天的Blog">
<meta property="og:description" content="记录网络模型的各种优化">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-10-23T08:14:15.000Z">
<meta property="article:modified_time" content="2024-06-16T08:10:22.836Z">
<meta property="article:author" content="周小天">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="正则化">
<meta property="article:tag" content="归一化">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="zchsakura.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)---%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%90%86%E8%A7%A3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>深度学习笔记(四)---优化方法理解 | 周小天的Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="周小天的Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">周小天的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">35</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">13</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">35</span></a>

  </li>
        <li class="menu-item menu-item-links">

    <a href="/links/" rel="section"><i class="fa fa-fw fa-link"></i>友链</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-rss">

    <a href="/atom.xml" rel="section"><i class="fa fa-fw fa-rss"></i>rss</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
	    <div class="site-search">
		  <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

	    </div>
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="zchsakura.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)---%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%90%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E4%B9%9D%E5%8D%81.jpeg">
      <meta itemprop="name" content="周小天">
      <meta itemprop="description" content="站台汽笛响起<br>想念是你的声音<br>我们提着过去走入人群">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周小天的Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          深度学习笔记(四)---优化方法理解
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-23 16:14:15" itemprop="dateCreated datePublished" datetime="2020-10-23T16:14:15+08:00">2020-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-06-16 16:10:22" itemprop="dateModified" datetime="2024-06-16T16:10:22+08:00">2024-06-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)---%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%90%86%E8%A7%A3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)---%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%90%86%E8%A7%A3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>记录网络模型的各种优化</p>
<a id="more"></a>
<h2 id="超参数">超参数</h2>
<p>比如算法中的<strong>learning rate</strong> $ a $
（学习率）、<strong>iterations</strong> (梯度下降法循环的数量) 、$ L $
（隐藏层数目）、$ n^{[l]} $ （隐藏层单元数目）、<strong>choice of
activation
function</strong>（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数$
W $ 和 $ b $
的值，所以它们被称作超参数。后面还有很多超参数我还有没学到，总的来讲就是能影响参数的更高级参数。</p>
<p>超参数的取值并没有明确的规定，这目前还是一个经验性的过程，新手上路最多的还是要不断的尝试，通过不断地修改超参数对比结果来不断修正最后找到一个相对合适的。</p>
<h2 id="偏差和方差">偏差和方差</h2>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201021152647.png"
alt="111" />
<figcaption aria-hidden="true">111</figcaption>
</figure>
<p>假设这就是数据集，如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（<strong>high
bias</strong>）的情况，我们称为“欠拟合”（<strong>underfitting</strong>）。</p>
<p>相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式分类器方差较高（<strong>high
variance</strong>），数据过度拟合（<strong>overfitting</strong>）。</p>
<h2 id="正则化">正则化</h2>
<p>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。</p>
<h3 id="l2范数">L2范数</h3>
<p><span class="math inline">\(\frac{\lambda}{2m}\)</span>乘以<span
class="math inline">\(w\)</span>范数的平方，其中<span
class="math inline">\(\left\| w \right\|_2^2\)</span>是<span
class="math inline">\(w\)</span>的欧几里德范数的平方，等于<span
class="math inline">\(w_{j}\)</span>（<span
class="math inline">\(j\)</span> 值从1到<span
class="math inline">\(n_{x}\)</span>）平方的和，也可表示为<span
class="math inline">\(w^{T}w\)</span>，也就是向量参数<span
class="math inline">\(w\)</span>
的欧几里德范数（2范数）的平方，此方法称为<span
class="math inline">\(L2\)</span>正则化，因为这里用了欧几里德范数，被称为向量参数<span
class="math inline">\(w\)</span>的<span
class="math inline">\(L2\)</span>范数。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201021162913.png"
alt="fa185e95684bbe6c0e9100164aff2ee5" />
<figcaption
aria-hidden="true">fa185e95684bbe6c0e9100164aff2ee5</figcaption>
</figure>
<p>因为<span
class="math inline">\(w\)</span>通常是一个高维参数矢量，已经可以表达高偏差问题，<span
class="math inline">\(w\)</span>可能包含有很多参数，我们不可能拟合所有参数，而<span
class="math inline">\(b\)</span>只是单个数字，所以<span
class="math inline">\(w\)</span>几乎涵盖所有参数，而不是<span
class="math inline">\(b\)</span>，如果加了参数<span
class="math inline">\(b\)</span>，其实也没太大影响，因为<span
class="math inline">\(b\)</span>只是众多参数中的一个，所以通常省略不计。</p>
<h3 id="l1范数">L1范数</h3>
<p>正则化，加的不是<span
class="math inline">\(L2\)</span>范数，而是正则项<span
class="math inline">\(\frac{\lambda}{m}\)</span>乘以<span
class="math inline">\(\sum_{j= 1}^{n_{x}}{|w|}\)</span>，<span
class="math inline">\(\sum_{j =1}^{n_{x}}{|w|}\)</span>也被称为参数<span
class="math inline">\(w\)</span>向量的<span
class="math inline">\(L1\)</span>范数，无论分母是<span
class="math inline">\(m\)</span>还是<span
class="math inline">\(2m\)</span>，它都是一个比例常量。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201022093917.png"
alt="5190dbc5a98db7a248499f54a04257cc" />
<figcaption
aria-hidden="true">5190dbc5a98db7a248499f54a04257cc</figcaption>
</figure>
<p>如果用的是<span class="math inline">\(L1\)</span>正则化，<span
class="math inline">\(w\)</span>最终会是稀疏的，也就是说<span
class="math inline">\(w\)</span>向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少。实际上，虽然<span
class="math inline">\(L1\)</span>正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是<span
class="math inline">\(L1\)</span>正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用<span
class="math inline">\(L2\)</span>正则化。</p>
<h3 id="弗罗贝尼乌斯范数矩阵l2范数">弗罗贝尼乌斯范数（矩阵L2范数）</h3>
<p>正则项为<span class="math inline">\(\frac{\lambda
}{2m}{\sum\nolimits_{1}^{L}|| W^{[l]}}||^2\)</span>，我们称<span
class="math inline">\(||W^{\left[l\right]}||^2\)</span>为范数平方，这个矩阵范数<span
class="math inline">\(||W^{\left[l\right]}||^2\)</span>（即平方范数），被定义为矩阵中所有元素的平方求和。该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标<span
class="math inline">\(F\)</span>标注”，鉴于线性代数中一些神秘晦涩的原因，我们不称之为“矩阵<span
class="math inline">\(L2\)</span>范数”，而称它为“弗罗贝尼乌斯范数”，矩阵<span
class="math inline">\(L2\)</span>范数听起来更自然，但鉴于一些大家无须知道的特殊原因，按照惯例，我们称之为“弗罗贝尼乌斯范数”，它表示一个矩阵中所有元素的平方和。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201022095410.png"
alt="a8336b2cfeed4128a23f20fab843d226" />
<figcaption
aria-hidden="true">a8336b2cfeed4128a23f20fab843d226</figcaption>
</figure>
<p>该如何使用该范数实现梯度下降呢？</p>
<p>这就是之前我们额外增加的正则化项，既然已经增加了这个正则项，现在我们要做的就是给<span
class="math inline">\(dW\)</span>加上这一项<span
class="math inline">\(\frac
{\lambda}{m}W^{[l]}\)</span>，然后计算这个更新项，使用新定义的<span
class="math inline">\(dW^{[l]}\)</span>，它的定义含有相关参数代价函数导数和，以及最后添加的额外正则项，这也是<span
class="math inline">\(L2\)</span>正则化有时被称为“权重衰减”的原因。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201022095606.png"
alt="dafb163da5b9c3ece677a7ebce05b680" />
<figcaption
aria-hidden="true">dafb163da5b9c3ece677a7ebce05b680</figcaption>
</figure>
<p>我们用$ dW<sup>{[l]}<span
class="math inline">\(的定义替换此处的\)</span>dW</sup>{[l]}<span
class="math inline">\(，可以看到，\)</span>W<sup>{[l]}<span
class="math inline">\(的定义被更新为\)</span>W</sup>{[l]}<span
class="math inline">\(减去学习率\)</span>$ 乘以<strong>backprop</strong>
再加上<span
class="math inline">\(\frac{\lambda}{m}W^{[l]}\)</span>。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201022095643.png"
alt="cba0f1c7a480139acb04e762e4fe57f8" />
<figcaption
aria-hidden="true">cba0f1c7a480139acb04e762e4fe57f8</figcaption>
</figure>
<p>该正则项说明，不论<span
class="math inline">\(W^{[l]}\)</span>是什么，我们都试图让它变得更小，实际上，相当于我们给矩阵W乘以<span
class="math inline">\((1 -
\alpha\frac{\lambda}{m})\)</span>倍的权重，矩阵<span
class="math inline">\(W\)</span>减去<span
class="math inline">\(\alpha\frac{\lambda}{m}\)</span>倍的它，也就是用这个系数<span
class="math inline">\((1-\alpha\frac{\lambda}{m})\)</span>乘以矩阵<span
class="math inline">\(W\)</span>，该系数小于1，因此<span
class="math inline">\(L2\)</span>范数正则化也被称为“权重衰减”，因为它就像一般的梯度下降，<span
class="math inline">\(W\)</span>被更新为少了<span
class="math inline">\(\alpha\)</span>乘以<strong>backprop</strong>输出的最初梯度值，同时<span
class="math inline">\(W\)</span>也乘以了这个系数，这个系数小于1，因此<span
class="math inline">\(L2\)</span>正则化也被称为“权重衰减”。</p>
<h3 id="dropout-正则化dropout-regularization">dropout 正则化（Dropout
Regularization）</h3>
<p>还有一个非常实用的正则化方法——“<strong>Dropout</strong>（随机失活）”，我们来看看它的工作原理。</p>
<p>dropout就是为每一层设置一个消除节点的概率，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。</p>
<p>直观上理解：不愿意把所有赌注都放在一个节点上，不愿意给任何一个输入加上太多权重，因为它可能会被删除，因此该单元将通过这种方式积极地传播开，并为单元的四个输入增加一点权重，通过传播所有权重，<strong>dropout</strong>将产生收缩权重的平方范数的效果，和之前讲的<span
class="math inline">\(L2\)</span>正则化类似。</p>
<p><strong>dropout</strong>一大缺点就是代价函数<span
class="math inline">\(J\)</span>不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。</p>
<h3 id="early-stopping">early stopping</h3>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201022101655.png"
alt="f5fd5df8235145c54aece1a5bf7b31f6" />
<figcaption
aria-hidden="true">f5fd5df8235145c54aece1a5bf7b31f6</figcaption>
</figure>
<p>就是让迭代提前停止，找到一个w大小适中的点</p>
<h2 id="为什么正则化能防止过拟合">为什么正则化能防止过拟合</h2>
<p>根据上面的正则化方法的描述我们可以看出正则化主要就是在控制参数W的大小，要让参数W不要太大，太复杂。直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，它会使这个网络从过度拟合的状态更接近高偏差状态。</p>
<p>但是<span
class="math inline">\(\lambda\)</span>会存在一个中间值，于是会有一个接近“<strong>Just
Right</strong>”的中间状态。</p>
<p>直观理解就是<span
class="math inline">\(\lambda\)</span>增加到足够大，<span
class="math inline">\(W\)</span>会接近于0，实际上是不会发生这种情况的，我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网络的所有隐藏单元依然存在，但是它们的影响变得更小了。神经网络变得更简单了，貌似这样更不容易发生过拟合。</p>
<p>其实这就是一个直觉经验，但是确实可以看到方差减小的结果。</p>
<h2 id="归一化输入normalizing-inputs">归一化输入（Normalizing
inputs）</h2>
<ul>
<li>零均值</li>
<li>归一化方差</li>
</ul>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201022101830.png"
alt="L2_week1_19" />
<figcaption aria-hidden="true">L2_week1_19</figcaption>
</figure>
<h2 id="梯度消失爆炸">梯度消失/爆炸</h2>
<p>深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于根据深度以指数方式变小，这加大了训练的难度。</p>
<h3 id="解决方法">解决方法</h3>
<p>这并不是完全的解决方法，但是可以减少影响。</p>
<p><span class="math inline">\(z = w_{1}x_{1} + w_{2}x_{2} + \ldots
+w_{n}x_{n}\)</span>，<span
class="math inline">\(b=0\)</span>，暂时忽略<span
class="math inline">\(b\)</span>，为了预防<span
class="math inline">\(z\)</span>值过大或过小，你可以看到<span
class="math inline">\(n\)</span>越大，你希望<span
class="math inline">\(w_{i}\)</span>越小，因为<span
class="math inline">\(z\)</span>是<span
class="math inline">\(w_{i}x_{i}\)</span>的和，如果你把很多此类项相加，希望每项值更小，最合理的方法就是设置<span
class="math inline">\(Var(w_{i})=\frac{1}{n}\)</span> 。即<span
class="math inline">\(w^{[l]} = np.random.randn(
\text{shape})*\text{np.}\text{sqrt}(\frac{1}{n^{[l-1]}})\)</span> ,
<span class="math inline">\(n^{[l - 1]}\)</span>就是我喂给第<span
class="math inline">\(l\)</span>层神经单元的数量（即第<span
class="math inline">\(l-1\)</span>层神经元数量）</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201022103557.png"
alt="e4114d7dc1c6242bd96cdadb457b8959" />
<figcaption
aria-hidden="true">e4114d7dc1c6242bd96cdadb457b8959</figcaption>
</figure>
<p>如果你是用的是<strong>Relu</strong>激活函数，而不是<span
class="math inline">\(\frac{1}{n}\)</span>，方差设置为<span
class="math inline">\(\frac{2}{n}\)</span>，效果会更好。</p>
<h2 id="mini-batch梯度下降">mini-batch梯度下降</h2>
<p>现在是大数据时代，训练集的样本数可能成百万上千万，如果每次正向传播一遍之后再反向进行梯度下降，那么一次传播的处理时间太慢了。</p>
<p>可以把训练集分割为小一点的子集训练，这些子集被取名为<strong>mini-batch</strong>，假设每一个子集中只有1000个样本，那么把其中的<span
class="math inline">\(x^{(1)}\)</span>到<span
class="math inline">\(x^{(1000)}\)</span>取出来，将其称为第一个子训练集，也叫做<strong>mini-batch</strong>，然后你再取出接下来的1000个样本，从<span
class="math inline">\(x^{(1001)}\)</span>到<span
class="math inline">\(x^{(2000)}\)</span>，然后再取1000个样本，以此类推。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023162136.png"
alt="112c45cf393d896833ffce29e14fe8bc" />
<figcaption
aria-hidden="true">112c45cf393d896833ffce29e14fe8bc</figcaption>
</figure>
<p>对<span
class="math inline">\(Y\)</span>也要进行相同处理，你也要相应地拆分<span
class="math inline">\(Y\)</span>的训练集，所以这是<span
class="math inline">\(Y^{\{1\}}\)</span>，然后从<span
class="math inline">\(y^{(1001)}\)</span>到<span
class="math inline">\(y^{(2000)}\)</span>，这个叫<span
class="math inline">\(Y^{\{2\}}\)</span>，一直到<span
class="math inline">\(Y^{\{ 5000\}}\)</span>。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023162155.png"
alt="609ea5be2140af929985951f2aab542c" />
<figcaption
aria-hidden="true">609ea5be2140af929985951f2aab542c</figcaption>
</figure>
<p>使用<strong>mini-batch</strong>梯度下降法，如果你作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是
$ X^{大括号t大括号}$ （后五个也有大括号，但是显示不出来）和<span
class="math inline">\(Y^{t}\)</span>，如果要作出成本函数<span
class="math inline">\(J^{t}\)</span>的图，而<span
class="math inline">\(J^{t}\)</span>只和<span
class="math inline">\(X^{t}\)</span>，<span
class="math inline">\(Y^{t}\)</span>有关，也就是每次迭代下你都在训练不同的样本集或者说训练不同的<strong>mini-batch</strong></p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023163405.png"
alt="b5c07d7dec7e54bed73cdcd43e79452d" />
<figcaption
aria-hidden="true">b5c07d7dec7e54bed73cdcd43e79452d</figcaption>
</figure>
<h2
id="指数加权平均数exponentially-weighted-averages">指数加权平均数（Exponentially
weighted averages）</h2>
<h3 id="什么是指数加权平均数">什么是指数加权平均数</h3>
<p>在我的理解里面这是一种参考当前时刻之前一部分时间段的内的数据的一中平均算法，这个时间段的长度由
<span class="math inline">\(\beta\)</span> 确定，时间长度大约为<span
class="math inline">\(\frac{1}{(1-\beta)}\)</span> 。</p>
<p>计算指数加权平均数的关键方程。</p>
<p><span class="math inline">\(v_t=\beta v_{t-1}+(1-\beta
)\theta_t\)</span></p>
<p><span
class="math inline">\(\beta=0.9\)</span>的时候，得到的结果是红线，如果它更接近于1，比如0.98，结果就是绿线，如果<span
class="math inline">\(\beta\)</span>小一点，如果是0.5，结果就是黄线。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023163627.png"
alt="6f5438fde578ef4285059d85976d52ed" />
<figcaption
aria-hidden="true">6f5438fde578ef4285059d85976d52ed</figcaption>
</figure>
<p>同样的公式，<span class="math inline">\({v}_{t}=\beta
{v}_{t-1}+(1-\beta ){\theta }_{t}\)</span></p>
<p>使<span
class="math inline">\(\beta=0.9\)</span>，写下相应的几个公式，所以在执行的时候，<span
class="math inline">\(t\)</span>从0到1到2到3，<span
class="math inline">\(t\)</span>的值在不断增加，为了更好地分析，我写的时候使得<span
class="math inline">\(t\)</span>的值不断减小，然后继续往下写。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023163648.png"
alt="9ab7565d5a3e13a9a525ec6d2f119a79" />
<figcaption
aria-hidden="true">9ab7565d5a3e13a9a525ec6d2f119a79</figcaption>
</figure>
<p>如果你把这些式子合并，</p>
<p><span class="math inline">\(v_{100} = 0.1\theta_{100} + 0.1 \times
0.9 \theta_{99} + 0.1 \times {(0.9)}^{2}\theta_{98} + 0.1 \times
{(0.9)}^{3}\theta_{97} + 0.1 \times {(0.9)}^{4}\theta_{96} +
\ldots\)</span></p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023163820.png"
alt="a3f97d59db202127014297ceccf1aacb" />
<figcaption
aria-hidden="true">a3f97d59db202127014297ceccf1aacb</figcaption>
</figure>
<p>计算<span
class="math inline">\(v_{100}\)</span>是通过，把两个函数对应的元素，然后求和，用这个数值100号数据值乘以0.1，99号数据值乘以0.1乘以<span
class="math inline">\({(0.9)}^{2}\)</span>，这是第二项，以此类推，所以选取的是每日温度，将其与指数衰减函数相乘，然后求和，就得到了<span
class="math inline">\(v_{100}\)</span>。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023163909.png"
alt="880dbafaccbd93a4ab0a92a9b8607956" />
<figcaption
aria-hidden="true">880dbafaccbd93a4ab0a92a9b8607956</figcaption>
</figure>
<p>指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了，正因为这个原因，其效率，它基本上只占用一行代码，计算指数加权平均数也只占用单行数字的存储和内存，当然它并不是最好的，也不是最精准的计算平均数的方法。如果你要计算移动窗，你直接算出过去10天的总和，过去50天的总和，除以10和50就好，如此往往会得到更好的估测。但缺点是，如果保存所有最近的温度数据，和过去10天的总和，必须占用更多的内存，执行更加复杂，计算成本也更加高昂。</p>
<h3
id="指数加权平均的偏差修正bias-correction-in-exponentially-weighted-averages">指数加权平均的偏差修正（Bias
correction in exponentially weighted averages）</h3>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023164324.png"
alt="26a3c3022a7f7ae7ba0cd27fc74cbcf6" />
<figcaption
aria-hidden="true">26a3c3022a7f7ae7ba0cd27fc74cbcf6</figcaption>
</figure>
<p>在实际使用中因为我们初始化<span class="math inline">\(v_{0} =
0\)</span>，所以在开始的时候可能会出现紫线的情况，与绿的的预估情况出现偏差，在前期不能很好的预测出当日温度。</p>
<p>有个办法可以修改这一估测，让估测变得更好，更准确，特别是在估测初期，也就是不用<span
class="math inline">\(v_{t}\)</span>，而是用<span
class="math inline">\(\frac{v_{t}}{1-
\beta^{t}}\)</span>，t就是现在的天数。举个具体例子，当<span
class="math inline">\(t=2\)</span>时，<span class="math inline">\(1 -
\beta^{t} = 1 -  {0.98}^{2} =
0.0396\)</span>，因此对第二天温度的估测变成了<span
class="math inline">\(\frac{v_{2}}{0.0396} =\frac{0.0196\theta_{1}
+  0.02\theta_{2}}{0.0396}\)</span>，也就是<span
class="math inline">\(\theta_{1}\)</span>和<span
class="math inline">\(\theta_{2}\)</span>的加权平均数，并去除了偏差。你会发现随着<span
class="math inline">\(t\)</span>增加，<span
class="math inline">\(\beta^{t}\)</span>接近于0，所以当<span
class="math inline">\(t\)</span>很大的时候，偏差修正几乎没有作用，因此当<span
class="math inline">\(t\)</span>较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023164640.png"
alt="f602c9d517a7f6c01fe18171dade17e6" />
<figcaption
aria-hidden="true">f602c9d517a7f6c01fe18171dade17e6</figcaption>
</figure>
<h2
id="动量梯度下降法gradient-descent-with-momentum">动量梯度下降法（Gradient
descent with Momentum）</h2>
<p>动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，简而言之，基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023165032.png"
alt="cc2d415b8ccda9fdaba12c575d4d3c4b" />
<figcaption
aria-hidden="true">cc2d415b8ccda9fdaba12c575d4d3c4b</figcaption>
</figure>
<p>在上图梯度中，你会发现这些纵轴上的摆动平均值接近于零，所以在纵轴方向，你希望放慢一点，平均过程中，正负数相互抵消，所以平均值接近于零。但在横轴方向，所有的微分都指向横轴方向，因此横轴方向的平均值仍然较大，因此用算法几次迭代后，你发现动量梯度下降法，最终纵轴方向的摆动变小了，横轴方向运动更快，因此你的算法走了一条更加直接的路径，在抵达最小值的路上减少了摆动。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023164850.png"
alt="89d51f9b5882cf226111b2c0eda2f3e3" />
<figcaption
aria-hidden="true">89d51f9b5882cf226111b2c0eda2f3e3</figcaption>
</figure>
<h2 id="rmsprop">RMSprop</h2>
<p>你们知道了动量（<strong>Momentum</strong>）可以加快梯度下降，还有一个叫做<strong>RMSprop</strong>的算法，全称是<strong>root
mean square prop</strong>算法，它也可以加速梯度下降。</p>
<p>回忆一下我们之前的例子，如果你执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅度摆动，为了分析这个例子，假设纵轴代表参数<span
class="math inline">\(b\)</span>，横轴代表参数<span
class="math inline">\(W\)</span>，可能有<span
class="math inline">\(W_{1}\)</span>，<span
class="math inline">\(W_{2}\)</span>或者其它重要的参数，为了便于理解，被称为<span
class="math inline">\(b\)</span>和<span
class="math inline">\(W\)</span>。</p>
<p>所以，你想减缓<span
class="math inline">\(b\)</span>方向的学习，即纵轴方向，同时加快，至少不是减缓横轴方向的学习，<strong>RMSprop</strong>算法可以实现这一点。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023171734.png"
alt="553ee26f6efd82d9996dec5f77e3f12e" />
<figcaption
aria-hidden="true">553ee26f6efd82d9996dec5f77e3f12e</figcaption>
</figure>
<p>记得在横轴方向或者在例子中的<span
class="math inline">\(W\)</span>方向，我们希望学习速度快，而在垂直方向，也就是例子中的<span
class="math inline">\(b\)</span>方向，我们希望减缓纵轴上的摆动，所以有了<span
class="math inline">\(S_{dW}\)</span>和<span
class="math inline">\(S_{db}\)</span>。你看这些微分，垂直方向的要比水平方向的大得多，所以斜率在<span
class="math inline">\(b\)</span>方向特别大，所以这些微分中，<span
class="math inline">\(db\)</span>较大，<span
class="math inline">\(dW\)</span>较小，因为函数的倾斜程度，在纵轴上，也就是b方向上要大于在横轴上，也就是<span
class="math inline">\(W\)</span>方向上。<span
class="math inline">\(db\)</span>的平方较大，所以<span
class="math inline">\(S_{db}\)</span>也会较大，而相比之下，<span
class="math inline">\(dW\)</span>会小一些，亦或<span
class="math inline">\(dW\)</span>平方会小一些，因此<span
class="math inline">\(S_{dW}\)</span>会小一些，结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。</p>
<p>如果<span
class="math inline">\(S_{dW}\)</span>的平方根趋近于0怎么办？得到的答案就非常大，为了确保数值稳定，在实际操练的时候，你要在分母上加上一个很小很小的<span
class="math inline">\(\varepsilon\)</span>，<span
class="math inline">\(\varepsilon\)</span>是多少没关系，<span
class="math inline">\(10^{-8}\)</span>是个不错的选择，这只是保证数值能稳定一些。</p>
<h2 id="adam-优化算法adam-optimization-algorithm">Adam 优化算法(Adam
optimization algorithm)</h2>
<p><strong>Adam</strong>优化算法基本上就是将<strong>Momentum</strong>和<strong>RMSprop</strong>结合在一起</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023172610.png"
alt="9ca9bfc160d53b23ea0d1164e6accffe" />
<figcaption
aria-hidden="true">9ca9bfc160d53b23ea0d1164e6accffe</figcaption>
</figure>
<p><strong>Adam</strong>算法结合了<strong>Momentum</strong>和<strong>RMSprop</strong>梯度下降法，并且是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。</p>
<p>本算法中有很多超参数，超参数学习率<span
class="math inline">\(a\)</span>很重要，也经常需要调试，你可以尝试一系列值，然后看哪个有效。<span
class="math inline">\(\beta_{1}\)</span>常用的缺省值为0.9，这是dW的移动平均数，也就是<span
class="math inline">\(dW\)</span>的加权平均数，这是<strong>Momentum</strong>涉及的项。至于超参数<span
class="math inline">\(\beta_{2}\)</span>，<strong>Adam</strong>论文作者，也就是<strong>Adam</strong>算法的发明者，推荐使用0.999，这是在计算<span
class="math inline">\({(dW)}^{2}\)</span>以及<span
class="math inline">\({(db)}^{2}\)</span>的移动加权平均值，关于<span
class="math inline">\(\varepsilon\)</span>的选择其实没那么重要，<strong>Adam</strong>论文的作者建议<span
class="math inline">\(\varepsilon\)</span>为<span
class="math inline">\(10^{-8}\)</span>，但你并不需要设置它，因为它并不会影响算法表现。但是在使用<strong>Adam</strong>的时候，人们往往使用缺省值即可，<span
class="math inline">\(\beta_{1}\)</span>，<span
class="math inline">\(\beta_{2}\)</span>和<span
class="math inline">\(\varepsilon\)</span>都是如此。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023172644.png"
alt="e9858303cd62eacc21759b16a121ff58" />
<figcaption
aria-hidden="true">e9858303cd62eacc21759b16a121ff58</figcaption>
</figure>
<h2 id="学习率衰减learning-rate-decay">学习率衰减(Learning rate
decay)</h2>
<p>慢慢减少<span
class="math inline">\(a\)</span>的本质在于，在学习初期，你能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023185637.png"
alt="7c4141aeec2ef4afc6aa534a486a34c8" />
<figcaption
aria-hidden="true">7c4141aeec2ef4afc6aa534a486a34c8</figcaption>
</figure>
<p>你应该拆分成不同的<strong>mini-batch</strong>，第一次遍历训练集叫做第一代。第二次就是第二代，依此类推，你可以将<span
class="math inline">\(a\)</span>学习率设为<span class="math inline">\(a=
\frac{1}{1 + decayrate *
\text{epoch}\text{-num}}a_{0}\)</span>（<strong>decay-rate</strong>称为衰减率，<strong>epoch-num</strong>为代数，<span
class="math inline">\(\alpha_{0}\)</span>为初始学习率），注意这个衰减率是另一个你需要调整的超参数。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023185644.png"
alt="7e0edfb697e8262dc39a040a987c62bd" />
<figcaption
aria-hidden="true">7e0edfb697e8262dc39a040a987c62bd</figcaption>
</figure>
<p>除了这个学习率衰减的公式，人们还会用其它的公式。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023185740.png"
alt="e1b6dc57b8b73ecf5ff400852c4f7086" />
<figcaption
aria-hidden="true">e1b6dc57b8b73ecf5ff400852c4f7086</figcaption>
</figure>
<h2 id="调试超参数">调试超参数</h2>
<p>最为广泛的学习应用是<span
class="math inline">\(a\)</span>，学习速率是需要调试的最重要的超参数。</p>
<p>除了<span
class="math inline">\(a\)</span>，还有一些参数需要调试，例如<strong>Momentum</strong>参数<span
class="math inline">\(\beta\)</span>，0.9就是个很好的默认值。我还会调试<strong>mini-batch</strong>的大小，以确保最优算法运行有效。我还会经常调试隐藏单元数目。</p>
<p>当应用<strong>Adam</strong>算法时，事实上，我从不调试<span
class="math inline">\(\beta_{1}\)</span>，<span
class="math inline">\({\beta}_{2}\)</span>和<span
class="math inline">\(\varepsilon\)</span>，我总是选定其分别为0.9，0.999和<span
class="math inline">\(10^{-8}\)</span>。</p>
<p>当你给超参数取值时，另一个惯例是采用由粗糙到精细的策略。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023190403.png"
alt="c3b248ac8ca2cf646d5b705270e01e78" />
<figcaption
aria-hidden="true">c3b248ac8ca2cf646d5b705270e01e78</figcaption>
</figure>
<p>比如在二维的那个例子中，你进行了取值，也许你会发现效果最好的某个点，也许这个点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域（小蓝色方框内），然后在其中更密集得取值或随机取值，聚集更多的资源，在这个蓝色的方格中搜索，如果你怀疑这些超参数在这个区域的最优结果，那在整个的方格中进行粗略搜索后，你会知道接下来应该聚焦到更小的方格中。在更小的方格中，你可以更密集得取点。所以这种从粗到细的搜索也经常使用。</p>
<h2
id="为超参数选择合适的范围using-an-appropriate-scale-to-pick-hyperparameters">为超参数选择合适的范围（Using
an appropriate scale to pick hyperparameters）</h2>
<p>在超参数范围中，随机取值可以提升你的搜索效率。但随机取值并不是在有效范围内的随机均匀取值，而是选择合适的标尺，用于探究这些超参数，这很重要。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023191011.png"
alt="651422f74439fd7e648e364d26d21485" />
<figcaption
aria-hidden="true">651422f74439fd7e648e364d26d21485</figcaption>
</figure>
<p>看看这个例子，假设你在搜索超参数<span
class="math inline">\(a\)</span>（学习速率），假设你怀疑其值最小是0.0001或最大是1。如果你画一条从0.0001到1的数轴，沿其随机均匀取值，那90%的数值将会落在0.1到1之间，结果就是，在0.1到1之间，应用了90%的资源，而在0.0001到0.1之间，只有10%的搜索资源，这看上去不太对。</p>
<p>反而，用对数标尺搜索超参数的方式会更合理，因此这里不使用线性轴，分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用，还有在0.001到0.01之间等等。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023191028.png"
alt="08307acea7c8db95c1479c611189dbfa" />
<figcaption
aria-hidden="true">08307acea7c8db95c1479c611189dbfa</figcaption>
</figure>
<p>所以在<strong>Python</strong>中，你可以这样做，使<code>r=-4*np.random.rand()</code>，然后<span
class="math inline">\(a\)</span>随机取值，$ a =10^{r}<span
class="math inline">\(，所以，第一行可以得出\)</span>r <span
class="math inline">\(，那么\)</span>a $，所以最左边的数字是<span
class="math inline">\(10^{-4}\)</span>，最右边是<span
class="math inline">\(10^{0}\)</span>。</p>
<h2
id="归一化网络的激活函数normalizing-activations-in-a-network">归一化网络的激活函数（Normalizing
activations in a network）</h2>
<p>对每一层的Z进行归一化</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023191837.png"
alt="e10ea98faa1ce9fe86ec8bf9f4fef71e" />
<figcaption
aria-hidden="true">e10ea98faa1ce9fe86ec8bf9f4fef71e</figcaption>
</figure>
<p>现在我们已把这些<span
class="math inline">\(z\)</span>值标准化，化为含平均值0和标准单位方差，所以<span
class="math inline">\(z\)</span>的每一个分量都含有平均值0和方差1，但我们不想让隐藏单元总是含有平均值0和方差1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算，我们称之为<span
class="math inline">\({\tilde{z}}^{(i)}\)</span>，<span
class="math inline">\({\tilde{z}}^{(i)}= \gamma z_{\text{norm}}^{(i)}
+\beta\)</span>，这里<span class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\beta\)</span>是你模型的学习参数，所以我们使用梯度下降或一些其它类似梯度下降的算法，比如<strong>Momentum</strong>或者<strong>Nesterov</strong>，<strong>Adam</strong>，你会更新<span
class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\beta\)</span>，正如更新神经网络的权重一样。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023191908.png"
alt="3d494a1d3fb25f7fd67f1bdbf8d8e464" />
<figcaption
aria-hidden="true">3d494a1d3fb25f7fd67f1bdbf8d8e464</figcaption>
</figure>
<p>请注意<span class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\beta\)</span>的作用是，你可以随意设置<span
class="math inline">\({\tilde{z}}^{(i)}\)</span>的平均值，事实上，如果<span
class="math inline">\(\gamma= \sqrt{\sigma^{2}
+\varepsilon}\)</span>，如果<span
class="math inline">\(\gamma\)</span>等于这个分母项（<span
class="math inline">\(z_{\text{norm}}^{(i)} = \frac{z^{(i)}
-\mu}{\sqrt{\sigma^{2} +\varepsilon}}\)</span>中的分母），<span
class="math inline">\(\beta\)</span>等于<span
class="math inline">\(\mu\)</span>，这里的这个值是<span
class="math inline">\(z_{\text{norm}}^{(i)}= \frac{z^{(i)} -
\mu}{\sqrt{\sigma^{2} + \varepsilon}}\)</span>中的<span
class="math inline">\(\mu\)</span>，那么<span
class="math inline">\(\gamma z_{\text{norm}}^{(i)}
+\beta\)</span>的作用在于，它会精确转化这个方程，如果这些成立（<span
class="math inline">\(\gamma =\sqrt{\sigma^{2} + \varepsilon},\beta
=\mu\)</span>），那么<span class="math inline">\({\tilde{z}}^{(i)} =
z^{(i)}\)</span>。</p>
<p>通过对<span class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\beta\)</span>合理设定，规范化过程，即这四个等式，从根本来说，只是计算恒等函数，通过赋予<span
class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\beta\)</span>其它值，可以使你构造含其它平均值和方差的隐藏单元值。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023191937.png"
alt="6216dd90d3483f05f08bd8dc86fc7df6" />
<figcaption
aria-hidden="true">6216dd90d3483f05f08bd8dc86fc7df6</figcaption>
</figure>
<h2 id="softmax-回归softmax-regression">Softmax 回归（Softmax
regression）</h2>
<p><strong>Softmax</strong>回归主要用来做多分类任务</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023192500.png"
alt="e65ba7b81d0b02d021c33bf0094f4059" />
<figcaption
aria-hidden="true">e65ba7b81d0b02d021c33bf0094f4059</figcaption>
</figure>
<p>让你的网络做到这一点的标准模型要用到<strong>Softmax</strong>层，以及输出层来生成输出，让我把式子写下来，然后回过头来，就会对<strong>Softmax</strong>的作用有一点感觉了。</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023192523.png"
alt="97ab8f2af0788776bdd486c5f4f40354" />
<figcaption
aria-hidden="true">97ab8f2af0788776bdd486c5f4f40354</figcaption>
</figure>
<p>损失函数：</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023192609.png"
alt="ed6ccb8dc9e65953f383a3bb774e8f53" />
<figcaption
aria-hidden="true">ed6ccb8dc9e65953f383a3bb774e8f53</figcaption>
</figure>
<p>梯度下降：</p>
<figure>
<img
src="http://zchsakura-blog.oss-cn-beijing.aliyuncs.com/20201023192636.png"
alt="c357c15e4133152bd8bb262789e71765" />
<figcaption
aria-hidden="true">c357c15e4133152bd8bb262789e71765</figcaption>
</figure>

    </div>

    
    
    
     <!-- 文章结束表示语-->
    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本 文 结 束 啦 <i class="fa fa-paw"></i> 感 谢 您 的 阅 读-------------</div>
    
</div>

      
    </div>
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\深度学习笔记(三)---基础理解\" rel="bookmark">深度学习笔记(三)---基础理解</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\深度学习笔记(一)---基础\" rel="bookmark">深度学习笔记(一)---基础</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\深度学习笔记(二)---框架\" rel="bookmark">深度学习笔记(二)---框架</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\深度学习笔记(零)---英语\" rel="bookmark">深度学习笔记(零)---英语</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/WeChatpay.jpg" alt="周小天 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>周小天
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://zchsakura.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)---%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%90%86%E8%A7%A3/" title="深度学习笔记(四)---优化方法理解">zchsakura.github.io/深度学习笔记(四)---优化方法理解/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

            <div class="social-item">
              <a target="_blank" class="social-link" href="/atom.xml">
                <span class="icon">
                  <i class="fa fa-rss"></i>
                </span>

                <span class="label">RSS</span>
              </a>
            </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
              <a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" rel="tag"><i class="fa fa-tag"></i> 正则化</a>
              <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" rel="tag"><i class="fa fa-tag"></i> 归一化</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%89)---%E5%9F%BA%E7%A1%80%E7%90%86%E8%A7%A3/" rel="prev" title="深度学习笔记(三)---基础理解">
      <i class="fa fa-chevron-left"></i> 深度学习笔记(三)---基础理解
    </a></div>
      <div class="post-nav-item">
    <a href="/%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E5%90%8C%E6%AD%A5%E5%8D%9A%E5%AE%A2/" rel="next" title="多台电脑同步博客">
      多台电脑同步博客 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数"><span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差和方差"><span class="nav-text">偏差和方差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#l2范数"><span class="nav-text">L2范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#l1范数"><span class="nav-text">L1范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#弗罗贝尼乌斯范数矩阵l2范数"><span class="nav-text">弗罗贝尼乌斯范数（矩阵L2范数）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropout-正则化dropout-regularization"><span class="nav-text">dropout 正则化（Dropout
Regularization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#early-stopping"><span class="nav-text">early stopping</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么正则化能防止过拟合"><span class="nav-text">为什么正则化能防止过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化输入normalizing-inputs"><span class="nav-text">归一化输入（Normalizing
inputs）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度消失爆炸"><span class="nav-text">梯度消失&#x2F;爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方法"><span class="nav-text">解决方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mini-batch梯度下降"><span class="nav-text">mini-batch梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#指数加权平均数exponentially-weighted-averages"><span class="nav-text">指数加权平均数（Exponentially
weighted averages）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是指数加权平均数"><span class="nav-text">什么是指数加权平均数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数加权平均的偏差修正bias-correction-in-exponentially-weighted-averages"><span class="nav-text">指数加权平均的偏差修正（Bias
correction in exponentially weighted averages）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#动量梯度下降法gradient-descent-with-momentum"><span class="nav-text">动量梯度下降法（Gradient
descent with Momentum）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rmsprop"><span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adam-优化算法adam-optimization-algorithm"><span class="nav-text">Adam 优化算法(Adam
optimization algorithm)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率衰减learning-rate-decay"><span class="nav-text">学习率衰减(Learning rate
decay)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#调试超参数"><span class="nav-text">调试超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为超参数选择合适的范围using-an-appropriate-scale-to-pick-hyperparameters"><span class="nav-text">为超参数选择合适的范围（Using
an appropriate scale to pick hyperparameters）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化网络的激活函数normalizing-activations-in-a-network"><span class="nav-text">归一化网络的激活函数（Normalizing
activations in a network）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax-回归softmax-regression"><span class="nav-text">Softmax 回归（Softmax
regression）</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="周小天"
      src="/images/%E4%B9%9D%E5%8D%81.jpeg">
  <p class="site-author-name" itemprop="name">周小天</p>
  <div class="site-description" itemprop="description">站台汽笛响起<br>想念是你的声音<br>我们提着过去走入人群</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ZCHSakura" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ZCHSakura" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:724353935@qq.com" title="E-Mail → mailto:724353935@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://kezhi.tech/" title="https:&#x2F;&#x2F;kezhi.tech&#x2F;" rel="noopener" target="_blank">于小鸡的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.kezhi.tech/linux-command/" title="https:&#x2F;&#x2F;www.kezhi.tech&#x2F;linux-command&#x2F;" rel="noopener" target="_blank">Linux命令查询</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.jianshu.com/p/191d1e21f7ed/" title="https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;191d1e21f7ed&#x2F;" rel="noopener" target="_blank">Markdown基本语法</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zhuanlan.zhihu.com/p/110756681" title="https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;110756681" rel="noopener" target="_blank">LeTaX公式</a>
        </li>
    </ul>
  </div>

      </div>

      <!--数字时钟-->
      
      
      
      <div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>
      
	  
	  <!--音乐播放器-->
	  <!--
		<div>
		  <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="https://music.163.com/outchain/player?type=2&id=1412266089&auto=1&height=66"></iframe>
        </div>
	  -->
      
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">周小天</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">127k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:55</span>
</div>




    <script async src="https://busuanzi.icodeq.com/busuanzi.pure.mini.js"></script>
    本文总阅读量 <span id="busuanzi_value_page_pv"></span> 次
    <span class="post-meta-divider">|</span>
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
    <span class="post-meta-divider">|</span>
    本站访客数<span id="busuanzi_value_site_uv"></span>人


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'FnNdu3eMMy5MBI9uBtoGkOq8-gzGzoHsz',
      appKey     : 'eaqzpiD8qTAmamfDoRSzQqnA',
      placeholder: "在这里输入评论内容，请不要忘记留下邮箱地址以便接收通知^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '5' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>




  

  <script async src="/js/cursor/fireworks.js"></script>




  <script src="/js/wobblewindow.js"></script>
  <script>
    //只在桌面版网页启用特效
    if( window.innerWidth > 768  ){
      $(document).ready(function () {
        
          $('#header').wobbleWindow({
            radius: 50,
            movementTop: false,
            movementLeft: false,
            movementRight: false,
            debug: false,
          });
        

        
          $('#sidebar').wobbleWindow({
            radius: 50,
            movementLeft: false,
            movementTop: false,
            movementBottom: false,
            position: 'fixed',
            debug: false,
          });
        

        
          $('#footer').wobbleWindow({
            radius: 50,
            movementBottom: false,
            movementLeft: false,
            movementRight: false,
            offsetX: ,
            position: 'absolute',
            debug: false,
          });
        
      });
    }
  </script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/ Relative","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":200,"height":270},"mobile":{"show":false},"react":{"opacityDefault":0.7}});</script></body>
</html>
